{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIMZtmKsEplTeXAPWGAZgr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baeseungyou/study/blob/main/%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%A0%95%EB%B3%B4%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOrGLGc9s_eE",
        "outputId": "48b0d078-44a9-4d45-b1dc-eb64f8fc7fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "#샘플 텍스트\n",
        "txt = \"자연어 처리는 인공지능의 중요한 분야 중 하나입니다. 이는 인간의 언어를 이해하고 생성하는 기술입니다. 이 과정에는 다양한 알고리즘이 사용됩니다.\"\n",
        "\n",
        "#nltk 데이터 다운로드 (최초 1회 필요)\n",
        "nltk.download('punkt')\n",
        "\n",
        "#문장 토큰화\n",
        "sentences = sent_tokenize(txt) # Changed 'text' to 'txt'\n",
        "\n",
        "#결과 출력\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"문장 {i}: {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPo_olYVuzPs",
        "outputId": "cce264e8-167b-4e3e-b02e-73f1e145aa77"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 1: 자연어 처리는 인공지능의 중요한 분야 중 하나입니다.\n",
            "문장 2: 이는 인간의 언어를 이해하고 생성하는 기술입니다.\n",
            "문장 3: 이 과정에는 다양한 알고리즘이 사용됩니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#샘플 텍스트\n",
        "text = \"자연어 처리는 인공지능의 중요한 분야입니다.\"\n",
        "\n",
        "#nltk 데이터 다운로드 (최초 1회 필요)\n",
        "nltk.download('punkt')\n",
        "\n",
        "#단어 토큰화\n",
        "words = word_tokenize(text)\n",
        "\n",
        "#결과 출력\n",
        "for i, word in enumerate(words, 1):\n",
        "    print(f\"단어 {i}: {word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yldq710uyJYF",
        "outputId": "684f2637-93c6-4619-e4a9-d67a1a436345"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 1: 자연어\n",
            "단어 2: 처리는\n",
            "단어 3: 인공지능의\n",
            "단어 4: 중요한\n",
            "단어 5: 분야입니다\n",
            "단어 6: .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#샘플 텍스트\n",
        "text = \"<html>자연어 처리는 매우 중요합니다!!! 😊 #인공지능 </html>\"\n",
        "\n",
        "#1. HTML 태그 제거\n",
        "def remove_html(text):\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  return soup.get_text()\n",
        "\n",
        "#2. 특수 문자 제거\n",
        "def remove_special_characters(text):\n",
        "  #알파벳, 한글, 숫자를 제외한 나머지 문자 제거\n",
        "  return re.sub(r'[^a-zA-Z0-9가-힣\\s]', '', text)\n",
        "\n",
        "#3. 이모티콘 제거\n",
        "def remove_emojis(text):\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "                        u\"\\0001F600-\\U0001F64F\" #이모티콘\n",
        "                        u\"\\0001F300-\\U0001F5FF\" #기호 & 피토그래프\n",
        "                        u\"\\0001F680-\\U0001F6FF\" # 교통 & 국기\n",
        "                        u\"\\0001F1E0-\\U0001F1FF\" # 국기\n",
        "                            \"] + \", flags=re.UNICODE)\n",
        "  return emoji_pattern.sub(r'', text)"
      ],
      "metadata": {
        "id": "503G3YOKylt0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKN9yStv5UxJ",
        "outputId": "c752651c-5e4e-4de3-b482-145e0cd79a96"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hcf5Low07EaL",
        "outputId": "30066d2c-2ae4-4fe5-d525-a06e50342bae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "#샘플 문장들\n",
        "sentences = [\n",
        "    \"나는 자연어 처리를 좋아합니다\",\n",
        "    \"자연어 처리는 매우 재미있습니다\",\n",
        "    \"나는 자연어 처리를 배우고 있습니다\"\n",
        "]\n",
        "\n",
        "#Tokenizer 객체 생성\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "#단어 집합을 생성하고 각 단어에 고유한 숫자를 할당\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "#텍스트를 정수 시퀀스로 변환\n",
        "encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "#단어 집합 출력\n",
        "word_index = tokenizer.word_index\n",
        "print(\"단어 집합:\", word_index)\n",
        "\n",
        "#정수 인코딩 결과 출력\n",
        "print(\"정수 인코딩 결과:\", encoded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE9wxg-m7PE6",
        "outputId": "00ddb8af-ff70-4810-80d8-dea87f05f764"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합: {'자연어': 1, '나는': 2, '처리를': 3, '좋아합니다': 4, '처리는': 5, '매우': 6, '재미있습니다': 7, '배우고': 8, '있습니다': 9}\n",
            "정수 인코딩 결과: [[2, 1, 3, 4], [1, 5, 6, 7], [2, 1, 3, 8, 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#샘플 문장들\n",
        "sentences = [\n",
        "    \"나는 자연어 처리를 좋아합니다\",\n",
        "    \"자연어 처리는 매우 재미있습니다\",\n",
        "    \"나는 자연어 처리를 배우고 있습니다\"\n",
        "]\n",
        "\n",
        "#Tokenizer 객체 생성 및 단어 집합 생성\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "#텍스트를 정수 시퀀스로 변환\n",
        "encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "#정수 인코딩 결과 출력\n",
        "print(\"정수 인코딩 결과:\", encoded_sentences)\n",
        "\n",
        "#패딩 적용\n",
        "padded_sentences = pad_sequences(encoded_sentences, padding='post')\n",
        "\n",
        "#패딩 적용 결과 출력\n",
        "print(\"패딩 적용 후 결과: \", padded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4tcACkC8EJ4",
        "outputId": "2af43ce1-7412-43b7-ea67-8f6a509324ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 결과: [[2, 1, 3, 4], [1, 5, 6, 7], [2, 1, 3, 8, 9]]\n",
            "패딩 적용 후 결과:  [[2 1 3 4 0]\n",
            " [1 5 6 7 0]\n",
            " [2 1 3 8 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PFUENbBV9go2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#샘플 문장들\n",
        "sentences = [\n",
        "    \"나는 자연어 처리를 좋아합니다\",\n",
        "    \"자연어 처리는 매우 재미있습니다\",\n",
        "    \"나는 자연어 처리를 배우고 있습니다\"\n",
        "]\n",
        "\n",
        "# Tokenizer 객체 생성 및 단어 집합 생성\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "#텍스트를 정수 시퀀스로 변환\n",
        "encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "#단어 집합 출력\n",
        "word_index = tokenizer.word_index\n",
        "print(\"단어 집합:\", word_index)\n",
        "\n",
        "#정수 인코딩된 첫 번째 문장에 대한 원-핫 인코딩 적용\n",
        "encoded_sentence = encoded_sentences[0]\n",
        "one_hot_encoded = to_categorical(encoded_sentence, num_classes=len(word_index) + 1) #인덱스가 1부터 시작하므로\n",
        "\n",
        "#원-핫 인코딩 결과 출력\n",
        "print(\"정수 인코딩 결과:\", encoded_sentence)\n",
        "print(\"원-핫 인코딩 결과:\\n\", one_hot_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEUAfPPi9GtP",
        "outputId": "a87bf1cf-627e-4e73-a540-c84bc9362cf1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합: {'자연어': 1, '나는': 2, '처리를': 3, '좋아합니다': 4, '처리는': 5, '매우': 6, '재미있습니다': 7, '배우고': 8, '있습니다': 9}\n",
            "정수 인코딩 결과: [2, 1, 3, 4]\n",
            "원-핫 인코딩 결과:\n",
            " [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    }
  ]
}
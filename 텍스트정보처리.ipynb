{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6UZdpn3z3UY/4IAgl0sDP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baeseungyou/study/blob/main/%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%A0%95%EB%B3%B4%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOrGLGc9s_eE",
        "outputId": "48b0d078-44a9-4d45-b1dc-eb64f8fc7fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "#ìƒ˜í”Œ í…ìŠ¤íŠ¸\n",
        "txt = \"ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ ì¤‘ìš”í•œ ë¶„ì•¼ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ëŠ” ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ ê³¼ì •ì—ëŠ” ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.\"\n",
        "\n",
        "#nltk ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ í•„ìš”)\n",
        "nltk.download('punkt')\n",
        "\n",
        "#ë¬¸ì¥ í† í°í™”\n",
        "sentences = sent_tokenize(txt) # Changed 'text' to 'txt'\n",
        "\n",
        "#ê²°ê³¼ ì¶œë ¥\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"ë¬¸ì¥ {i}: {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPo_olYVuzPs",
        "outputId": "cce264e8-167b-4e3e-b02e-73f1e145aa77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë¬¸ì¥ 1: ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ ì¤‘ìš”í•œ ë¶„ì•¼ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\n",
            "ë¬¸ì¥ 2: ì´ëŠ” ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
            "ë¬¸ì¥ 3: ì´ ê³¼ì •ì—ëŠ” ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#ìƒ˜í”Œ í…ìŠ¤íŠ¸\n",
        "text = \"ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ ì¤‘ìš”í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.\"\n",
        "\n",
        "#nltk ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ í•„ìš”)\n",
        "nltk.download('punkt')\n",
        "\n",
        "#ë‹¨ì–´ í† í°í™”\n",
        "words = word_tokenize(text)\n",
        "\n",
        "#ê²°ê³¼ ì¶œë ¥\n",
        "for i, word in enumerate(words, 1):\n",
        "    print(f\"ë‹¨ì–´ {i}: {word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yldq710uyJYF",
        "outputId": "684f2637-93c6-4619-e4a9-d67a1a436345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë‹¨ì–´ 1: ìì—°ì–´\n",
            "ë‹¨ì–´ 2: ì²˜ë¦¬ëŠ”\n",
            "ë‹¨ì–´ 3: ì¸ê³µì§€ëŠ¥ì˜\n",
            "ë‹¨ì–´ 4: ì¤‘ìš”í•œ\n",
            "ë‹¨ì–´ 5: ë¶„ì•¼ì…ë‹ˆë‹¤\n",
            "ë‹¨ì–´ 6: .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#ìƒ˜í”Œ í…ìŠ¤íŠ¸\n",
        "text = \"<html>ìì—°ì–´ ì²˜ë¦¬ëŠ” ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤!!! ğŸ˜Š #ì¸ê³µì§€ëŠ¥ </html>\"\n",
        "\n",
        "#1. HTML íƒœê·¸ ì œê±°\n",
        "def remove_html(text):\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  return soup.get_text()\n",
        "\n",
        "#2. íŠ¹ìˆ˜ ë¬¸ì ì œê±°\n",
        "def remove_special_characters(text):\n",
        "  #ì•ŒíŒŒë²³, í•œê¸€, ìˆ«ìë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë¬¸ì ì œê±°\n",
        "  return re.sub(r'[^a-zA-Z0-9ê°€-í£\\s]', '', text)\n",
        "\n",
        "#3. ì´ëª¨í‹°ì½˜ ì œê±°\n",
        "def remove_emojis(text):\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "                        u\"\\0001F600-\\U0001F64F\" #ì´ëª¨í‹°ì½˜\n",
        "                        u\"\\0001F300-\\U0001F5FF\" #ê¸°í˜¸ & í”¼í† ê·¸ë˜í”„\n",
        "                        u\"\\0001F680-\\U0001F6FF\" # êµí†µ & êµ­ê¸°\n",
        "                        u\"\\0001F1E0-\\U0001F1FF\" # êµ­ê¸°\n",
        "                            \"] + \", flags=re.UNICODE)\n",
        "  return emoji_pattern.sub(r'', text)"
      ],
      "metadata": {
        "id": "503G3YOKylt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKN9yStv5UxJ",
        "outputId": "c752651c-5e4e-4de3-b482-145e0cd79a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hcf5Low07EaL",
        "outputId": "30066d2c-2ae4-4fe5-d525-a06e50342bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "#ìƒ˜í”Œ ë¬¸ì¥ë“¤\n",
        "sentences = [\n",
        "    \"ë‚˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤\",\n",
        "    \"ìì—°ì–´ ì²˜ë¦¬ëŠ” ë§¤ìš° ì¬ë¯¸ìˆìŠµë‹ˆë‹¤\",\n",
        "    \"ë‚˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ë¥¼ ë°°ìš°ê³  ìˆìŠµë‹ˆë‹¤\"\n",
        "]\n",
        "\n",
        "#Tokenizer ê°ì²´ ìƒì„±\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "#ë‹¨ì–´ ì§‘í•©ì„ ìƒì„±í•˜ê³  ê° ë‹¨ì–´ì— ê³ ìœ í•œ ìˆ«ìë¥¼ í• ë‹¹\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "#í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
        "encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "#ë‹¨ì–´ ì§‘í•© ì¶œë ¥\n",
        "word_index = tokenizer.word_index\n",
        "print(\"ë‹¨ì–´ ì§‘í•©:\", word_index)\n",
        "\n",
        "#ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼ ì¶œë ¥\n",
        "print(\"ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼:\", encoded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE9wxg-m7PE6",
        "outputId": "00ddb8af-ff70-4810-80d8-dea87f05f764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë‹¨ì–´ ì§‘í•©: {'ìì—°ì–´': 1, 'ë‚˜ëŠ”': 2, 'ì²˜ë¦¬ë¥¼': 3, 'ì¢‹ì•„í•©ë‹ˆë‹¤': 4, 'ì²˜ë¦¬ëŠ”': 5, 'ë§¤ìš°': 6, 'ì¬ë¯¸ìˆìŠµë‹ˆë‹¤': 7, 'ë°°ìš°ê³ ': 8, 'ìˆìŠµë‹ˆë‹¤': 9}\n",
            "ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼: [[2, 1, 3, 4], [1, 5, 6, 7], [2, 1, 3, 8, 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#ìƒ˜í”Œ ë¬¸ì¥ë“¤\n",
        "sentences = [\n",
        "    \"ë‚˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤\",\n",
        "    \"ìì—°ì–´ ì²˜ë¦¬ëŠ” ë§¤ìš° ì¬ë¯¸ìˆìŠµë‹ˆë‹¤\",\n",
        "    \"ë‚˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ë¥¼ ë°°ìš°ê³  ìˆìŠµë‹ˆë‹¤\"\n",
        "]\n",
        "\n",
        "#Tokenizer ê°ì²´ ìƒì„± ë° ë‹¨ì–´ ì§‘í•© ìƒì„±\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "#í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
        "encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "#ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼ ì¶œë ¥\n",
        "print(\"ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼:\", encoded_sentences)\n",
        "\n",
        "#íŒ¨ë”© ì ìš©\n",
        "padded_sentences = pad_sequences(encoded_sentences, padding='post')\n",
        "\n",
        "#íŒ¨ë”© ì ìš© ê²°ê³¼ ì¶œë ¥\n",
        "print(\"íŒ¨ë”© ì ìš© í›„ ê²°ê³¼: \", padded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4tcACkC8EJ4",
        "outputId": "2af43ce1-7412-43b7-ea67-8f6a509324ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼: [[2, 1, 3, 4], [1, 5, 6, 7], [2, 1, 3, 8, 9]]\n",
            "íŒ¨ë”© ì ìš© í›„ ê²°ê³¼:  [[2 1 3 4 0]\n",
            " [1 5 6 7 0]\n",
            " [2 1 3 8 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#ìƒ˜í”Œ ë¬¸ì¥ë“¤\n",
        "sentences = [\n",
        "    \"ë‚˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤\",\n",
        "    \"ìì—°ì–´ ì²˜ë¦¬ëŠ” ë§¤ìš° ì¬ë¯¸ìˆìŠµë‹ˆë‹¤\",\n",
        "    \"ë‚˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ë¥¼ ë°°ìš°ê³  ìˆìŠµë‹ˆë‹¤\"\n",
        "]\n",
        "\n",
        "# Tokenizer ê°ì²´ ìƒì„± ë° ë‹¨ì–´ ì§‘í•© ìƒì„±\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "#í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
        "encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "#ë‹¨ì–´ ì§‘í•© ì¶œë ¥\n",
        "word_index = tokenizer.word_index\n",
        "print(\"ë‹¨ì–´ ì§‘í•©:\", word_index)\n",
        "\n",
        "#ì •ìˆ˜ ì¸ì½”ë”©ëœ ì²« ë²ˆì§¸ ë¬¸ì¥ì— ëŒ€í•œ ì›-í•« ì¸ì½”ë”© ì ìš©\n",
        "encoded_sentence = encoded_sentences[0]\n",
        "one_hot_encoded = to_categorical(encoded_sentence, num_classes=len(word_index) + 1) #ì¸ë±ìŠ¤ê°€ 1ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ\n",
        "\n",
        "#ì›-í•« ì¸ì½”ë”© ê²°ê³¼ ì¶œë ¥\n",
        "print(\"ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼:\", encoded_sentence)\n",
        "print(\"ì›-í•« ì¸ì½”ë”© ê²°ê³¼:\\n\", one_hot_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEUAfPPi9GtP",
        "outputId": "a87bf1cf-627e-4e73-a540-c84bc9362cf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë‹¨ì–´ ì§‘í•©: {'ìì—°ì–´': 1, 'ë‚˜ëŠ”': 2, 'ì²˜ë¦¬ë¥¼': 3, 'ì¢‹ì•„í•©ë‹ˆë‹¤': 4, 'ì²˜ë¦¬ëŠ”': 5, 'ë§¤ìš°': 6, 'ì¬ë¯¸ìˆìŠµë‹ˆë‹¤': 7, 'ë°°ìš°ê³ ': 8, 'ìˆìŠµë‹ˆë‹¤': 9}\n",
            "ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼: [2, 1, 3, 4]\n",
            "ì›-í•« ì¸ì½”ë”© ê²°ê³¼:\n",
            " [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "e01Ve7UoCyn_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4d62c10-566a-4cef-9c26-34f8303a487b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# 1. íŒŒì¼ ì½ê¸°\n",
        "file_path = \"C:/2ì£¼ì°¨_í…ìŠ¤íŠ¸.csv\"  # íŒŒì¼ ê²½ë¡œ ìˆ˜ì •: \\ë¥¼ /ë¡œ ë³€ê²½ ë˜ëŠ” \\\\ ì‚¬ìš©\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# ë°ì´í„° í™•ì¸\n",
        "print(\"íŒŒì¼ ë‚´ìš© í™•ì¸:\")\n",
        "print(data.head())\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ (ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •)\n",
        "text_data = data.iloc[:, 0].dropna().tolist()  # ê²°ì¸¡ì¹˜ ì œê±° í›„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
        "\n",
        "# 2. í† í°í™”, í‘œì œì–´ ì¶”ì¶œ, ë¶ˆìš©ì–´ ì²˜ë¦¬\n",
        "okt = Okt()\n",
        "stopwords = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ë„', 'ìœ¼ë¡œ', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ']\n",
        "\n",
        "# ê° ë¬¸ì¥ì„ í† í°í™”í•˜ê³  ë¶ˆìš©ì–´ë¥¼ ì œê±°í•œ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
        "tokenized_sentences = []\n",
        "for sentence in text_data:\n",
        "    # í˜•íƒœì†Œ ë¶„ì„ ë° í† í°í™”\n",
        "    tokens = okt.morphs(sentence)\n",
        "    # ë¶ˆìš©ì–´ ì œê±°\n",
        "    tokens = [word for word in tokens if word not in stopwords]\n",
        "    tokenized_sentences.append(tokens)\n",
        "\n",
        "# í† í°í™” ê²°ê³¼ ì¶œë ¥\n",
        "print(\"\\ní† í°í™” ë° ë¶ˆìš©ì–´ ì œê±° ê²°ê³¼:\")\n",
        "for idx, tokens in enumerate(tokenized_sentences):\n",
        "    print(f\"ë¬¸ì¥ {idx + 1}: {tokens}\")\n",
        "\n",
        "# 3. ì •ìˆ˜ ì¸ì½”ë”©\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokenized_sentences)\n",
        "encoded_sentences = tokenizer.texts_to_sequences(tokenized_sentences)\n",
        "\n",
        "# ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼ ì¶œë ¥\n",
        "print(\"\\nì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼:\")\n",
        "for idx, encoded in enumerate(encoded_sentences):\n",
        "    print(f\"ë¬¸ì¥ {idx + 1}: {encoded}\")\n",
        "\n",
        "# í˜•íƒœì†Œ-ì •ìˆ˜ ë§¤í•‘ ì •ë³´ ì¶œë ¥\n",
        "print(\"\\ní˜•íƒœì†Œ-ì •ìˆ˜ ë§¤í•‘ ì •ë³´:\")\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "# 4. íŒ¨ë”©\n",
        "max_length = max(len(sentence) for sentence in encoded_sentences)\n",
        "padded_sentences = pad_sequences(encoded_sentences, maxlen=max_length, padding='post')\n",
        "\n",
        "# íŒ¨ë”© ê²°ê³¼ ì¶œë ¥\n",
        "print(\"\\níŒ¨ë”© ê²°ê³¼:\")\n",
        "for idx, padded in enumerate(padded_sentences):\n",
        "    print(f\"ë¬¸ì¥ {idx + 1}: {padded}\")\n",
        "\n",
        "# 5. ì›-í•« ì¸ì½”ë”© (ì²« ë²ˆì§¸ ë¬¸ì¥ë§Œ ì²˜ë¦¬)\n",
        "first_sentence = encoded_sentences[0]\n",
        "num_words = len(tokenizer.word_index) + 1\n",
        "one_hot_encoding = np.eye(num_words)[first_sentence]\n",
        "\n",
        "# ì›-í•« ì¸ì½”ë”© ê²°ê³¼ ì¶œë ¥\n",
        "print(\"\\nì²« ë²ˆì§¸ ë¬¸ì¥ì˜ ì›-í•« ì¸ì½”ë”© ê²°ê³¼:\")\n",
        "print(one_hot_encoding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "PSK5Irujmdem",
        "outputId": "06858950-8827-41d8-d62e-da221e8c23a7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:/2ì£¼ì°¨_í…ìŠ¤íŠ¸.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7413d429ca46>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 1. íŒŒì¼ ì½ê¸°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"C:/2ì£¼ì°¨_í…ìŠ¤íŠ¸.csv\"\u001b[0m  \u001b[0;31m# íŒŒì¼ ê²½ë¡œ ìˆ˜ì •: \\ë¥¼ /ë¡œ ë³€ê²½ ë˜ëŠ” \\\\ ì‚¬ìš©\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# ë°ì´í„° í™•ì¸\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/2ì£¼ì°¨_í…ìŠ¤íŠ¸.csv'"
          ]
        }
      ]
    }
  ]
}
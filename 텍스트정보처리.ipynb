{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIMZtmKsEplTeXAPWGAZgr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baeseungyou/study/blob/main/%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%A0%95%EB%B3%B4%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOrGLGc9s_eE",
        "outputId": "48b0d078-44a9-4d45-b1dc-eb64f8fc7fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "#ìƒ˜í”Œ í…ìŠ¤íŠ¸\n",
        "txt = \"ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ ì¤‘ìš”í•œ ë¶„ì•¼ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ëŠ” ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ ê³¼ì •ì—ëŠ” ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.\"\n",
        "\n",
        "#nltk ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ í•„ìš”)\n",
        "nltk.download('punkt')\n",
        "\n",
        "#ë¬¸ì¥ í† í°í™”\n",
        "sentences = sent_tokenize(txt) # Changed 'text' to 'txt'\n",
        "\n",
        "#ê²°ê³¼ ì¶œë ¥\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"ë¬¸ì¥ {i}: {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPo_olYVuzPs",
        "outputId": "cce264e8-167b-4e3e-b02e-73f1e145aa77"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë¬¸ì¥ 1: ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ ì¤‘ìš”í•œ ë¶„ì•¼ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\n",
            "ë¬¸ì¥ 2: ì´ëŠ” ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
            "ë¬¸ì¥ 3: ì´ ê³¼ì •ì—ëŠ” ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#ìƒ˜í”Œ í…ìŠ¤íŠ¸\n",
        "text = \"ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ ì¤‘ìš”í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.\"\n",
        "\n",
        "#nltk ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ í•„ìš”)\n",
        "nltk.download('punkt')\n",
        "\n",
        "#ë‹¨ì–´ í† í°í™”\n",
        "words = word_tokenize(text)\n",
        "\n",
        "#ê²°ê³¼ ì¶œë ¥\n",
        "for i, word in enumerate(words, 1):\n",
        "    print(f\"ë‹¨ì–´ {i}: {word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yldq710uyJYF",
        "outputId": "684f2637-93c6-4619-e4a9-d67a1a436345"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë‹¨ì–´ 1: ìì—°ì–´\n",
            "ë‹¨ì–´ 2: ì²˜ë¦¬ëŠ”\n",
            "ë‹¨ì–´ 3: ì¸ê³µì§€ëŠ¥ì˜\n",
            "ë‹¨ì–´ 4: ì¤‘ìš”í•œ\n",
            "ë‹¨ì–´ 5: ë¶„ì•¼ì…ë‹ˆë‹¤\n",
            "ë‹¨ì–´ 6: .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#ìƒ˜í”Œ í…ìŠ¤íŠ¸\n",
        "text = \"<html>ìì—°ì–´ ì²˜ë¦¬ëŠ” ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤!!! ğŸ˜Š #ì¸ê³µì§€ëŠ¥ </html>\"\n",
        "\n",
        "#1. HTML íƒœê·¸ ì œê±°\n",
        "def remove_html(text):\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  return soup.get_text()\n",
        "\n",
        "#2. íŠ¹ìˆ˜ ë¬¸ì ì œê±°\n",
        "def remove_special_characters(text):\n",
        "  #ì•ŒíŒŒë²³, í•œê¸€, ìˆ«ìë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë¬¸ì ì œê±°\n",
        "  return re.sub(r'[^a-zA-Z0-9ê°€-í£\\s]', '', text)\n",
        "\n",
        "#3. ì´ëª¨í‹°ì½˜ ì œê±°\n",
        "def remove_emojis(text):\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "                        u\"\\0001F600-\\U0001F64F\" #ì´ëª¨í‹°ì½˜\n",
        "                        u\"\\0001F300-\\U0001F5FF\" #ê¸°í˜¸ & í”¼í† ê·¸ë˜í”„\n",
        "                        u\"\\0001F680-\\U0001F6FF\" # êµí†µ & êµ­ê¸°\n",
        "                        u\"\\0001F1E0-\\U0001F1FF\" # êµ­ê¸°\n",
        "                            \"] + \", flags=re.UNICODE)\n",
        "  return emoji_pattern.sub(r'', text)"
      ],
      "metadata": {
        "id": "503G3YOKylt0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKN9yStv5UxJ",
        "outputId": "c752651c-5e4e-4de3-b482-145e0cd79a96"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hcf5Low07EaL",
        "outputId": "30066d2c-2ae4-4fe5-d525-a06e50342bae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "#ìƒ˜í”Œ ë¬¸ì¥ë“¤\n",
        "sentences = [\n",
        "    \"ë‚˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤\",\n",
        "    \"ìì—°ì–´ ì²˜ë¦¬ëŠ” ë§¤ìš° ì¬ë¯¸ìˆìŠµë‹ˆë‹¤\",\n",
        "    \"ë‚˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ë¥¼ ë°°ìš°ê³  ìˆìŠµë‹ˆë‹¤\"\n",
        "]\n",
        "\n",
        "#Tokenizer ê°ì²´ ìƒì„±\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "#ë‹¨ì–´ ì§‘í•©ì„ ìƒì„±í•˜ê³  ê° ë‹¨ì–´ì— ê³ ìœ í•œ ìˆ«ìë¥¼ í• ë‹¹\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "#í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
        "encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "#ë‹¨ì–´ ì§‘í•© ì¶œë ¥\n",
        "word_index = tokenizer.word_index\n",
        "print(\"ë‹¨ì–´ ì§‘í•©:\", word_index)\n",
        "\n",
        "#ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼ ì¶œë ¥\n",
        "print(\"ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼:\", encoded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE9wxg-m7PE6",
        "outputId": "00ddb8af-ff70-4810-80d8-dea87f05f764"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë‹¨ì–´ ì§‘í•©: {'ìì—°ì–´': 1, 'ë‚˜ëŠ”': 2, 'ì²˜ë¦¬ë¥¼': 3, 'ì¢‹ì•„í•©ë‹ˆë‹¤': 4, 'ì²˜ë¦¬ëŠ”': 5, 'ë§¤ìš°': 6, 'ì¬ë¯¸ìˆìŠµë‹ˆë‹¤': 7, 'ë°°ìš°ê³ ': 8, 'ìˆìŠµë‹ˆë‹¤': 9}\n",
            "ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼: [[2, 1, 3, 4], [1, 5, 6, 7], [2, 1, 3, 8, 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#ìƒ˜í”Œ ë¬¸ì¥ë“¤\n",
        "sentences = [\n",
        "    \"ë‚˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤\",\n",
        "    \"ìì—°ì–´ ì²˜ë¦¬ëŠ” ë§¤ìš° ì¬ë¯¸ìˆìŠµë‹ˆë‹¤\",\n",
        "    \"ë‚˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ë¥¼ ë°°ìš°ê³  ìˆìŠµë‹ˆë‹¤\"\n",
        "]\n",
        "\n",
        "#Tokenizer ê°ì²´ ìƒì„± ë° ë‹¨ì–´ ì§‘í•© ìƒì„±\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "#í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
        "encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "#ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼ ì¶œë ¥\n",
        "print(\"ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼:\", encoded_sentences)\n",
        "\n",
        "#íŒ¨ë”© ì ìš©\n",
        "padded_sentences = pad_sequences(encoded_sentences, padding='post')\n",
        "\n",
        "#íŒ¨ë”© ì ìš© ê²°ê³¼ ì¶œë ¥\n",
        "print(\"íŒ¨ë”© ì ìš© í›„ ê²°ê³¼: \", padded_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4tcACkC8EJ4",
        "outputId": "2af43ce1-7412-43b7-ea67-8f6a509324ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼: [[2, 1, 3, 4], [1, 5, 6, 7], [2, 1, 3, 8, 9]]\n",
            "íŒ¨ë”© ì ìš© í›„ ê²°ê³¼:  [[2 1 3 4 0]\n",
            " [1 5 6 7 0]\n",
            " [2 1 3 8 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PFUENbBV9go2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#ìƒ˜í”Œ ë¬¸ì¥ë“¤\n",
        "sentences = [\n",
        "    \"ë‚˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤\",\n",
        "    \"ìì—°ì–´ ì²˜ë¦¬ëŠ” ë§¤ìš° ì¬ë¯¸ìˆìŠµë‹ˆë‹¤\",\n",
        "    \"ë‚˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ë¥¼ ë°°ìš°ê³  ìˆìŠµë‹ˆë‹¤\"\n",
        "]\n",
        "\n",
        "# Tokenizer ê°ì²´ ìƒì„± ë° ë‹¨ì–´ ì§‘í•© ìƒì„±\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "#í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
        "encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "#ë‹¨ì–´ ì§‘í•© ì¶œë ¥\n",
        "word_index = tokenizer.word_index\n",
        "print(\"ë‹¨ì–´ ì§‘í•©:\", word_index)\n",
        "\n",
        "#ì •ìˆ˜ ì¸ì½”ë”©ëœ ì²« ë²ˆì§¸ ë¬¸ì¥ì— ëŒ€í•œ ì›-í•« ì¸ì½”ë”© ì ìš©\n",
        "encoded_sentence = encoded_sentences[0]\n",
        "one_hot_encoded = to_categorical(encoded_sentence, num_classes=len(word_index) + 1) #ì¸ë±ìŠ¤ê°€ 1ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ\n",
        "\n",
        "#ì›-í•« ì¸ì½”ë”© ê²°ê³¼ ì¶œë ¥\n",
        "print(\"ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼:\", encoded_sentence)\n",
        "print(\"ì›-í•« ì¸ì½”ë”© ê²°ê³¼:\\n\", one_hot_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEUAfPPi9GtP",
        "outputId": "a87bf1cf-627e-4e73-a540-c84bc9362cf1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë‹¨ì–´ ì§‘í•©: {'ìì—°ì–´': 1, 'ë‚˜ëŠ”': 2, 'ì²˜ë¦¬ë¥¼': 3, 'ì¢‹ì•„í•©ë‹ˆë‹¤': 4, 'ì²˜ë¦¬ëŠ”': 5, 'ë§¤ìš°': 6, 'ì¬ë¯¸ìˆìŠµë‹ˆë‹¤': 7, 'ë°°ìš°ê³ ': 8, 'ìˆìŠµë‹ˆë‹¤': 9}\n",
            "ì •ìˆ˜ ì¸ì½”ë”© ê²°ê³¼: [2, 1, 3, 4]\n",
            "ì›-í•« ì¸ì½”ë”© ê²°ê³¼:\n",
            " [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    }
  ]
}